{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from transformers import DNABertTokenizer, DNABertForSequenceClassification\n",
    "from DNABERT.examples.transformers.data.processors.glue import DnaPromProcessor,glue_convert_examples_to_features\n",
    "from DNABERT.examples.transformers.data.processors.utils import InputExample\n",
    "from DNABERT.examples.run_finetune import TensorDataset\n",
    "from transformers import BertConfig,    DNATokenizer,    BertForSequenceClassification\n",
    "# 加载DNABERT模型和tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 测试\n",
    "# Read sequences from text file\n",
    "sequences_pos = [];sequences_neg = []\n",
    "with open('./dataset/your_test_set/positive.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        sequence = line.strip()\n",
    "        sequences_pos.append(sequence)\n",
    "with open('./dataset/your_test_set/negative.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        sequence = line.strip()\n",
    "        sequences_neg.append(sequence)\n",
    "\n",
    "df = pd.DataFrame({'name': [f'Sequence_{i}' for i, _ in enumerate(sequences_pos)],\n",
    "                   'sequence': sequences_pos})\n",
    "df['label']=1\n",
    "# print(df)\n",
    "df2 = pd.DataFrame({'name': [f'Sequence_{i}' for i, _ in enumerate(sequences_neg)],\n",
    "                   'sequence': sequences_neg})\n",
    "df2['label']=0\n",
    "df=pd.concat([df,df2],axis=0)\n",
    "def seq2kmer(seq):\n",
    "    k=3\n",
    "    kmer = [seq[x:x+k] for x in range(len(seq)+1-k)]\n",
    "    kmers = \" \".join(kmer)\n",
    "    return kmers\n",
    "def create_examples(df, set_type='dev'):\n",
    "    examples = [];i=1\n",
    "    for line in df.values:\n",
    "        guid = \"%s-%s\" % (set_type, i)\n",
    "        text_a = seq2kmer(line[0])\n",
    "        label = str(line[1])\n",
    "        examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        i=i+1\n",
    "    return examples\n",
    "def load_and_cache_examples(df):\n",
    "    used_model=\"./results/models/bert/N_snrna/checkpoint-7200\"# use your own model\n",
    "    tokenizer = DNATokenizer.from_pretrained(used_model)\n",
    "    processor = DnaPromProcessor()\n",
    "\n",
    "    label_list = processor.get_labels()\n",
    "    examples = (create_examples(df))   \n",
    "\n",
    "    max_length = 101\n",
    "    pad_on_left = 0\n",
    "    pad_token = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
    "    \n",
    "    features = glue_convert_examples_to_features(\n",
    "    examples,\n",
    "    tokenizer,\n",
    "    label_list=label_list,\n",
    "    max_length=max_length,\n",
    "    output_mode=\"classification\",\n",
    "    pad_on_left=pad_on_left, \n",
    "    pad_token=pad_token,\n",
    "    pad_token_segment_id=0)\n",
    "            \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    return all_input_ids, all_attention_mask, all_token_type_ids, all_labels\n",
    "model=BertForSequenceClassification.from_pretrained(\"used_model\").to(device)\n",
    "token=load_and_cache_examples(df[[\"sequence\",\"label\"]])\n",
    "\n",
    "\n",
    "class mydataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.df=df[[\"sequence\",\"label\"]]\n",
    "        self.token=load_and_cache_examples(self.df)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self,idx):\n",
    "        return tuple(tensor[idx] for tensor in self.token)\n",
    "test_dataset=mydataset(df)\n",
    "\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=256,shuffle=False,drop_last=False)\n",
    "model.eval()\n",
    "test_result=[]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx,i in enumerate(test_dataloader):\n",
    "        x= {\"input_ids\": i[0].to(device), \"attention_mask\": i[1].to(device), \"labels\": i[3].to(device)}\n",
    "        # print(x)\n",
    "        outputs=model(**x)\n",
    "        # print(outputs)\n",
    "        test_result.append(outputs[1])\n",
    "        # break\n",
    "test_result=torch.cat(test_result,dim=0)\n",
    "pred=test_result.softmax(dim=1).detach().cpu()\n",
    "pred=torch.argmax(pred,dim=1)\n",
    "y_true=df.label.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
    "print(accuracy_score(y_true,pred))\n",
    "print(precision_score(y_true,pred))\n",
    "print(recall_score(y_true,pred))\n",
    "print(f1_score(y_true,pred))\n",
    "print(roc_auc_score(y_true,pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
