{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing:\n",
    "    use ./dataset/fastp.sh to clean the sequences first.\n",
    "### Deduplication:\n",
    "    Remove duplicate sequences from all N-glycosylation and O-glycosylation sequences.\n",
    "\n",
    "\n",
    "### Alignment:\n",
    "Reference Sets:\n",
    "\n",
    "1. Rfam reference set (including sRNA, snRNA, snoRNA, mature miRNA, tRNA)\n",
    "2. miRbase miRNA precursor reference set\n",
    "3. ncRNA from Ensembl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate sequences from all N-glycosylation and O-glycosylation sequences.\n",
    "\n",
    "def read_fasta(file_path):\n",
    "    sequences = {}\n",
    "    current_id = None\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                current_id = line[1:]\n",
    "                sequences[current_id] = ''\n",
    "            else:\n",
    "                sequences[current_id] += line\n",
    "    return sequences\n",
    "\n",
    "def write_fasta(output_path, sequences):\n",
    "    with open(output_path, 'w') as file:\n",
    "        for seq_id, sequence in sequences.items():\n",
    "            file.write(f'>{sequence}\\n{seq_id}\\n')\n",
    "\n",
    "def remove_duplicate_sequences(input_path, output_path):\n",
    "    sequences = read_fasta(input_path)\n",
    "    unique_sequences = {sequence: seq_id for seq_id, sequence in sequences.items()}\n",
    "    write_fasta(output_path, unique_sequences)\n",
    "\n",
    "# example\n",
    "# fasta_input_file = 'C1_R1_clean.fa'\n",
    "# fasta_output_file = 'C1_R1_clean_nonre.fa'\n",
    "# remove_duplicate_sequences(fasta_input_file, fasta_output_file)\n",
    "\n",
    "# for i in range(1, 4):\n",
    "#     fasta_input_file = 'C' + str(i) + '_R1_clean.fa'\n",
    "#     fasta_output_file = 'C' + str(i) + '_R1_clean_nonre.fa'\n",
    "#     remove_duplicate_sequences(fasta_input_file, fasta_output_file)\n",
    "#     fasta_input_file = 'N' + str(i) + '_R1_clean.fa'\n",
    "#     fasta_output_file = 'N' + str(i) + '_R1_clean_nonre.fa'\n",
    "#     remove_duplicate_sequences(fasta_input_file, fasta_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLAST\n",
    "maybe take a lot of time.\n",
    "#### First, cd to the directory \"./dataset/blast\" and run the following command to build the blast database:\n",
    "#### bash set_blastdb.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['O','N']:\n",
    "    # for j in ['srna','trna','rrna','snrna','rrna','hairpin']:\n",
    "    for j in ['pirna']:\n",
    "        print(i,j)\n",
    "        !blastn -query ./dataset/{i}/all_nonre.fa -db ./dataset/blastdb/rfam.hg.{j} -outfmt 6 -num_threads 20 -out ./dataset/{i}/{j}-short.txt -perc_identity 100 -task blastn-short -word_size 7 -evalue 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain a dataset without duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "def get_reverse_complement(seq):\n",
    "    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'N': 'N','U': 'A','A': 'U'}\n",
    "    return ''.join([complement[base] for base in seq[::-1]])\n",
    "def fasta2dict(fasta_path):\n",
    "    fa={}\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        fa[record.id]=str(record.seq)\n",
    "    return fa\n",
    "N_fa=fasta2dict(\"../\"+'N'+\"/\"+\"all_nonre.fa\")\n",
    "O_fa=fasta2dict(\"../\"+'O'+\"/\"+\"all_nonre.fa\")\n",
    "def get_set(types,t,k):\n",
    "\n",
    "    blast_file='../'+types+'/'+t+k+\".txt\"\n",
    "    # please download the reference file from the supplementary files\n",
    "    full_fa=fasta2dict(\"../ref/rfam.hg.\"+t+\".fa\")\n",
    "\n",
    "    if types==\"N\":\n",
    "        short_fa=N_fa\n",
    "    else:\n",
    "        short_fa=O_fa\n",
    "    short_set=[]\n",
    "    long_set=[]\n",
    "    with open(blast_file, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2: \n",
    "                # print(parts[0], short_fa[parts[0]],parts[1],full_fa[parts[1]])\n",
    "                short=short_fa[parts[0]];long=full_fa[parts[1]]\n",
    "                if t==\"hairpin\":\n",
    "                    short=short.replace(\"T\",\"U\")\n",
    "                if short in long:\n",
    "                    # print(\"yes\")\n",
    "                    short_set.append(short)\n",
    "                    long_set.append(long)\n",
    "                else:\n",
    "                    if get_reverse_complement(short) in long:\n",
    "                        short_set.append(get_reverse_complement(short))\n",
    "                        long_set.append(long)\n",
    "    print(\"未经过去重的正例数量：\",len(short_set),len(long_set))\n",
    "    short_set=list(set(short_set))\n",
    "    long_set=list(set(long_set))\n",
    "    print(\"经过去重后的正例数量：短序列：\",len(short_set),\"全长：\",len(long_set))\n",
    "\n",
    "    import random\n",
    "    import os\n",
    "    positive_samples = long_set\n",
    "    long_set = set(long_set)\n",
    "\n",
    "    # 使用集合差集操作找出不在long_set中的key对应的value\n",
    "    all_negative_samples = {value for key, value in full_fa.items() if key not in long_set}\n",
    "\n",
    "    #  transform short_set to set and judge whether all elements are not in short_set by set intersection\n",
    "    short_set = set(short_set)\n",
    "    negative_samples_sel = [i for i in all_negative_samples if not short_set.intersection(i)]\n",
    "\n",
    "\n",
    "    #  random select the same number of negative samples as positive samples\n",
    "    if len(negative_samples_sel) < len(positive_samples):\n",
    "        print(\"负例数量不足\")\n",
    "        negative_samples = negative_samples_sel\n",
    "    else:\n",
    "        \n",
    "        negative_samples = random.sample(negative_samples_sel, len(positive_samples))\n",
    "    if os.path.exists(\"../set/\"+types+\"/\"+t+k+\"/\"):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(\"../set/\"+types+\"/\"+t+k+\"/\")\n",
    "    with open(\"../set/\"+types+\"/\"+t+k+'/positive.txt', 'w') as pos_file:\n",
    "        for sample in positive_samples:\n",
    "            pos_file.write(sample + '\\n')\n",
    "\n",
    "    with open(\"../set/\"+types+\"/\"+t+k+'/negative.txt', 'w') as neg_file:\n",
    "        for sample in negative_samples:\n",
    "            neg_file.write(sample + '\\n')\n",
    "# for t in [\"srna\",\"hairpin\",\"snrna\",\"trna\",'rrna']:\n",
    "for t in [\"pirna\"]:\n",
    "    for k in [\"-short\"]:\n",
    "        print(t+k)\n",
    "        print(\"N\")\n",
    "        get_set(\"N\",t,k)\n",
    "        print(\"O\")\n",
    "        get_set(\"O\",t,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate sequences\n",
    "\n",
    "def convert_to_fasta(file_name, label, output_file):\n",
    "    with open(file_name, 'r') as file:\n",
    "        sequences = file.readlines()\n",
    "\n",
    "    with open(output_file, 'a') as fasta_file:\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            sequence = sequence.strip()  # Remove any trailing newline characters\n",
    "            if 'N' in sequence:\n",
    "                \n",
    "                continue\n",
    "            print( 'N' in sequence)\n",
    "            header = f\">sequence{i+1}|{label}|training\\n\"\n",
    "            fasta_file.write(header)\n",
    "            fasta_file.write(sequence + \"\\n\")\n",
    "root_path=\"./dataset/xxxx\"\n",
    "# Convert positive.txt with label 1\n",
    "convert_to_fasta(root_path+\"/positive.txt\", 1, root_path+\"/output.fasta\")\n",
    "\n",
    "# Convert negative.txt with label 0\n",
    "convert_to_fasta(root_path+\"/negative.txt\", 0, root_path+\"/output.fasta\")\n",
    "\n",
    "print(\"Conversion to FASTA format completed.\")\n",
    "\n",
    "!cd-hit-est -i ./dataset/O/all_nonre.fa -o ./dataset/O/CD-all_nonre.fa -M 0 -T 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "# example\n",
    "# cd ./dataset/O/\n",
    "with open('./CD-all_nonre.fasta', 'r') as fasta_file:\n",
    "    sequences = list(SeqIO.parse(fasta_file, 'fasta'))\n",
    "\n",
    "#  created new positive.txt and negative.txt files\n",
    "with open('./positive.txt', 'w') as pos_file, open('./positive.txt', 'w') as neg_file:\n",
    "    for seq in sequences:\n",
    "\n",
    "        if 'positive' in seq.description:\n",
    "            pos_file.write(str(seq.seq) + '\\n')\n",
    "        elif 'negative' in seq.description:\n",
    "            neg_file.write(str(seq.seq) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you can use the positive.txt and negative.txt files to train your model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnano",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
